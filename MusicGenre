{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hc4Z_HbnlGip"},"outputs":[],"source":["# Mount to Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EFDFE5MKkyg6"},"outputs":[],"source":["# Import and install needed Python libraries\n","!pip install librosa\n","!pip install pydub\n","!pip install keras-nlp\n","!pip install keras>=3\n","\n","from keras.preprocessing import image\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import sys"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SfqffGY_5XWn"},"outputs":[],"source":["# Meredith:\n","#%cd drive/MyDrive/'Deep Learning Project'/Code\n","\n","#Pranav:\n","# %cd drive/MyDrive/'Senior Fall'/'Deep Learning'/'Deep Learning Project'/Code\n","\n","#Pranav2:\n","%cd drive/MyDrive/'Colab Notebooks'/Code"]},{"cell_type":"code","source":["# Split songs to increase dataset size\n","\n","from pydub import AudioSegment\n","import os\n","\n","def splitSong(start, end, song, path):\n","    split = song[start:end]\n","    split.export(path, format=\"wav\")\n","\n","t0, t1, t2, t3 = 0, 10000, 20000, 30000\n","directory = \"Data/genres_original\"\n","\n","for i, folder in enumerate(os.listdir(directory)):\n","    path = directory + \"/\" + folder\n","    if folder != \".DS_Store\":\n","        os.mkdir(\"Data/genres_split/\" + folder)\n","        for file in os.listdir(path):\n","            if file != \".DS_Store\":\n","                songPath = \"Data/genres_split/\" + folder + \"/\" + file\n","                newSong = AudioSegment.from_wav(path + \"/\" + file)\n","                splitSong(t0,t1,newSong,songPath + \"0\")\n","                splitSong(t1,t2,newSong,songPath + \"1\")\n","                splitSong(t2,t3,newSong,songPath + \"3\")"],"metadata":{"id":"SF97v4CfvYlm"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ciipw0WpmmhE"},"outputs":[],"source":["# Import the dataset and convert data to spectrograms\n","\n","import os\n","import librosa\n","\n","directory = \"Data/genres_split\"\n","x, y = [], []\n","labels = {}\n","\n","for i, folder in enumerate(os.listdir(directory)):\n","  path = directory + '/' + folder\n","  if folder != '.DS_Store':\n","    labels[i] = folder\n","    if not os.path.isdir(\"Data/spectrograms/\" + folder):\n","      os.mkdir(\"Data/spectrograms/\" + folder)\n","    for file in os.listdir(path):\n","      if file != '.DS_Store':\n","        save_path = \"Data/spectrograms/\" + folder + '/' + file\n","        save_path = save_path.replace('.wav', '')\n","        save_path += '.png'\n","        audio, sr = librosa.load(path + '/' + file)\n","        D = np.abs(librosa.stft(audio))**2\n","        audio = librosa.feature.melspectrogram(y=audio, sr=sr, S=D)\n","        log_ms = librosa.power_to_db(audio, ref=np.max)\n","        audio = np.array(log_ms)[:, :428]\n","\n","        x.append(audio)\n","        y.append(i)\n","\n","        if (not os.path.isfile(save_path)):\n","          fig = plt.figure()\n","          ax = fig.add_subplot(1, 1, 1)\n","          fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n","          librosa.display.specshow(log_ms, sr=sr)\n","          plt.close()\n","\n","          fig.savefig(save_path)\n","\n","          audio, sr = librosa.load(path + '/' + file)\n","          D = np.abs(librosa.stft(audio))**2\n","          audio = librosa.feature.melspectrogram(y=audio, sr=sr, S=D)\n","          log_ms = librosa.power_to_db(audio, ref=np.max)"]},{"cell_type":"code","source":["x = np.stack(x, axis = 0)\n","\n","y = np.stack(y, axis = 0)"],"metadata":{"id":"DmEqfhomoSm4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4trFUtbcoswD"},"outputs":[],"source":["# Test/Train Split\n","\n","x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.2, random_state=0)\n","\n","x_train_norm = np.array(x_train) / 255\n","x_test_norm = np.array(x_test) / 255\n","\n","y_train_encoded = to_categorical(y_train)\n","y_test_encoded = to_categorical(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FOga7krgBZQX"},"outputs":[],"source":["# Create Model\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.models import Sequential\n","from keras.layers import LSTM\n","from keras.layers import Dropout\n","from keras.layers import Dense\n","from keras.layers import TimeDistributed\n","from keras.layers import Conv1D\n","from keras.layers import MaxPool1D\n","from keras.layers import Conv2D\n","from keras.layers import MaxPool2D\n","from keras.layers import Flatten\n","from keras.layers import LeakyReLU\n","from keras.layers import BatchNormalization\n","from tensorflow.keras.applications import ResNet50\n","from tensorflow.keras.applications import ResNet101\n","from tensorflow.keras.applications import ResNet50V2\n","from keras_nlp.layers import TransformerEncoder\n","\n","model = Sequential()\n","# model.add(LSTM(10,activation=\"tanh\",return_sequences=True,kernel_regularizer=keras.regularizers.l2(0.035)))\n","# model.add(Dropout(0.3))\n","# model.add(LSTM(10,activation=\"tanh\"))\n","# model.add(Dropout(0.3))\n","# model.add(Dense(10, activation='relu'))\n","\n","# model.add(LSTM(128,activation=\"tanh\",return_sequences=True,kernel_regularizer=keras.regularizers.l2(0.035)))\n","# model.add(Dropout(0.3))\n","# model.add(LSTM(128,activation=\"tanh\"))\n","# model.add(Dropout(0.3))\n","# model.add(Dense(10, activation='relu'))\n","\n","# model.add(LSTM(32,return_sequences=True,kernel_regularizer=keras.regularizers.l2(0.035)))\n","# model.add(Dropout(0.3))\n","# model.add(LSTM(32))\n","# model.add(Dropout(0.3))\n","# model.add(Dense(10,activation = \"softmax\"))\n","\n","# model.add(TransformerEncoder(128, 10, dropout = 0.5))\n","# model.add(Flatten())\n","# model.add(Dense(10, activation=\"softmax\"))\n","\n","# model.add(Conv1D(128, 3, activation=\"relu\"))\n","# model.add(MaxPool1D())\n","# model.add(Dropout(0.3))\n","# model.add(Conv1D(64, 3, activation=\"relu\"))\n","# model.add(MaxPool1D())\n","# model.add(Dropout(0.3))\n","# model.add(Conv1D(32, 3, activation=\"relu\"))\n","# model.add(MaxPool1D())\n","# model.add(Dropout(0.3))\n","# model.add(Flatten())\n","# model.add(Dense(10, activation=\"softmax\"))\n","\n","# model.add(Conv1D(256, 3, activation=\"relu\"))\n","# model.add(MaxPool1D())\n","# model.add(Dropout(0.3))\n","# model.add(Conv1D(128, 3, activation=\"relu\"))\n","# model.add(MaxPool1D())\n","# model.add(Dropout(0.3))\n","# model.add(Conv1D(64, 3, activation=\"relu\"))\n","# model.add(MaxPool1D())\n","# model.add(Dropout(0.3))\n","# model.add(Flatten())\n","# model.add(Dense(10, activation=\"softmax\"))\n","\n","# model.add(Conv1D(256, 3, activation=\"relu\"))\n","# model.add(MaxPool1D())\n","# model.add(Dropout(0.3))\n","# model.add(Conv1D(128, 3, activation=\"relu\"))\n","# model.add(MaxPool1D())\n","# model.add(Dropout(0.3))\n","# model.add(Flatten())\n","# model.add(Dense(10, activation=\"softmax\"))\n","\n","# model.add(Conv1D(512, 3, activation=\"relu\"))\n","# model.add(MaxPool1D())\n","# model.add(Dropout(0.3))\n","# model.add(Conv1D(256, 3, activation=\"relu\"))\n","# model.add(MaxPool1D())\n","# model.add(Dropout(0.3))\n","# model.add(Flatten())\n","# model.add(Dense(10, activation=\"softmax\"))\n","\n","# model.add(Conv2D(128, 3, activation=\"relu\", data_format=\"channels_last\"))\n","# model.add(MaxPool2D())\n","# model.add(Dropout(0.5))\n","# model.add(Conv2D(64, 3, activation=\"relu\", data_format=\"channels_last\"))\n","# model.add(MaxPool2D())\n","# model.add(Dropout(0.5))\n","# model.add(Conv2D(32, 3, activation=\"relu\", data_format=\"channels_last\"))\n","# model.add(MaxPool2D())\n","# model.add(Dropout(0.5))\n","# model.add(Conv2D(16, 3, activation=\"relu\", data_format=\"channels_last\"))\n","# model.add(MaxPool2D())\n","# model.add(Dropout(0.5))\n","# model.add(Conv2D(8, 3, activation=\"relu\", data_format=\"channels_last\"))\n","# model.add(MaxPool2D())\n","# model.add(Dropout(0.5))\n","# model.add(Flatten())\n","# model.add(Dense(10, activation=\"softmax\"))\n","\n","# model.add(Conv2D(256, 3, activation=\"relu\", data_format=\"channels_last\"))\n","# model.add(MaxPool2D())\n","# model.add(Dropout(0.5))\n","# model.add(Conv2D(128, 3, activation=\"relu\", data_format=\"channels_last\"))\n","# model.add(MaxPool2D())\n","# model.add(Dropout(0.5))\n","# model.add(Conv2D(64, 3, activation=\"relu\", data_format=\"channels_last\"))\n","# model.add(MaxPool2D())\n","# model.add(Dropout(0.5))\n","# model.add(Conv2D(32, 3, activation=\"relu\", data_format=\"channels_last\"))\n","# model.add(MaxPool2D())\n","# model.add(Dropout(0.5))\n","# model.add(Conv2D(16, 3, activation=\"relu\", data_format=\"channels_last\"))\n","# model.add(MaxPool2D())\n","# model.add(Dropout(0.5))\n","# model.add(Flatten())\n","# model.add(Dense(10, activation=\"softmax\"))\n","\n","# model.add(Conv2D(64, 3, activation=\"relu\", data_format=\"channels_last\"))\n","# model.add(MaxPool2D())\n","# model.add(Dropout(0.5))\n","# model.add(Conv2D(32, 3, activation=\"relu\", data_format=\"channels_last\"))\n","# model.add(MaxPool2D())\n","# model.add(Dropout(0.5))\n","# model.add(Conv2D(16, 3, activation=\"relu\", data_format=\"channels_last\"))\n","# model.add(MaxPool2D())\n","# model.add(Dropout(0.5))\n","# model.add(Conv2D(8, 3, activation=\"relu\", data_format=\"channels_last\"))\n","# model.add(MaxPool2D())\n","# model.add(Dropout(0.5))\n","# model.add(Conv2D(4, 3, activation=\"relu\", data_format=\"channels_last\"))\n","# model.add(MaxPool2D())\n","# model.add(Dropout(0.5))\n","# model.add(Flatten())\n","# model.add(Dense(10, activation=\"softmax\"))\n","\n","# model.add(Conv2D(256, 3, activation=\"relu\", data_format=\"channels_last\"))\n","# model.add(LeakyReLU())\n","# model.add(BatchNormalization())\n","# model.add(Conv2D(128, 3, activation=\"relu\", data_format=\"channels_last\"))\n","# model.add(LeakyReLU())\n","# model.add(BatchNormalization())\n","# model.add(Flatten())\n","# model.add(Dense(10, activation=\"softmax\"))\n","\n","# model.add(Conv1D(512, 3, activation=\"relu\"))\n","# model.add(LeakyReLU())\n","# model.add(BatchNormalization())\n","# model.add(Conv1D(256, 3, activation=\"relu\"))\n","# model.add(LeakyReLU())\n","# model.add(BatchNormalization())\n","# model.add(Flatten())\n","# model.add(Dense(10, activation=\"softmax\"))\n","\n","# model.add(Conv1D(1024, 3, activation=\"relu\"))\n","# model.add(LeakyReLU())\n","# model.add(BatchNormalization())\n","# model.add(Conv1D(512, 3, activation=\"relu\"))\n","# model.add(LeakyReLU())\n","# model.add(BatchNormalization())\n","# model.add(Flatten())\n","# model.add(Dense(10, activation=\"softmax\"))\n","\n","# model.add(Conv1D(256, 3, activation=\"relu\"))\n","# model.add(LeakyReLU())\n","# model.add(BatchNormalization())\n","# model.add(Conv1D(128, 3, activation=\"relu\"))\n","# model.add(LeakyReLU())\n","# model.add(BatchNormalization())\n","# model.add(Flatten())\n","# model.add(Dense(10, activation=\"softmax\"))\n","\n","# model.add(Conv1D(128, 3, activation=\"relu\"))\n","# model.add(LeakyReLU())\n","# model.add(BatchNormalization())\n","# model.add(Conv1D(64, 3, activation=\"relu\"))\n","# model.add(LeakyReLU())\n","# model.add(BatchNormalization())\n","# model.add(Conv1D(32, 3, activation=\"relu\"))\n","# model.add(LeakyReLU())\n","# model.add(BatchNormalization())\n","# model.add(Conv1D(16, 3, activation=\"relu\"))\n","# model.add(LeakyReLU())\n","# model.add(BatchNormalization())\n","# model.add(Conv1D(8, 3, activation=\"relu\"))\n","# model.add(LeakyReLU())\n","# model.add(BatchNormalization())\n","# model.add(Flatten())\n","# model.add(Dense(10, activation=\"softmax\"))\n","\n","# model.add(ResNet50(input_shape=x_train_norm.shape[1:], weights=None, classes = 10, classifier_activation=\"softmax\"))\n","\n","# model.add(ResNet101(input_shape=x_train_norm.shape[1:], weights=None, classes = 10, classifier_activation=\"softmax\"))\n","\n","# model.add(ResNet50V2(input_shape=x_train_norm.shape[1:], weights=None, classes = 10, classifier_activation=\"softmax\"))\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'], run_eagerly=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eq8s8jXsDFXh"},"outputs":[],"source":["# Train\n","import gc\n","gc.collect()\n","hist = model.fit(x_train_norm, y_train_encoded, validation_data=(x_test_norm, y_test_encoded), batch_size=32, epochs=100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ocl9G6OVDQHT"},"outputs":[],"source":["# Plot Train/Validation Accuracy\n","\n","acc = hist.history['accuracy']\n","val_acc = hist.history['val_accuracy']\n","epochs = range(1, len(acc) + 1)\n","\n","plt.plot(epochs, acc, '-', label='Training Accuracy')\n","plt.plot(epochs, val_acc, ':', label='Validation Accuracy')\n","plt.title('Training and Validation Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend(loc='lower right')\n","plt.plot()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"esfljPWzDcu6"},"outputs":[],"source":["# Confusion Matrix\n","\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","sns.set()\n","\n","y_predicted = model.predict(x_test_norm)\n","mat = confusion_matrix(y_test_encoded.argmax(axis=1), y_predicted.argmax(axis=1))\n","class_labels = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n","\n","sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',\n","            xticklabels=class_labels,\n","            yticklabels=class_labels)\n","\n","plt.xlabel('Predicted label')\n","plt.ylabel('Actual label')"]}],"metadata":{"colab":{"provenance":[],"gpuType":"V100","machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}