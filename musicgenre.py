# -*- coding: utf-8 -*-
"""MusicGenre

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AP5A9BxXDdWVAdW7EeaGydnRy0XpgjEX
"""

# Mount to Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Import and install needed Python libraries
!pip install librosa
!pip install pydub
!pip install keras-nlp
!pip install keras>=3

from keras.preprocessing import image
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
import sys

# Commented out IPython magic to ensure Python compatibility.
# Meredith:
#%cd drive/MyDrive/'Deep Learning Project'/Code

#Pranav:
# %cd drive/MyDrive/'Senior Fall'/'Deep Learning'/'Deep Learning Project'/Code

#Pranav2:
# %cd drive/MyDrive/'Colab Notebooks'/Code

# Split songs to increase dataset size

from pydub import AudioSegment
import os

def splitSong(start, end, song, path):
    split = song[start:end]
    split.export(path, format="wav")

t0, t1, t2, t3 = 0, 10000, 20000, 30000
directory = "Data/genres_original"

for i, folder in enumerate(os.listdir(directory)):
    path = directory + "/" + folder
    if folder != ".DS_Store":
        os.mkdir("Data/genres_split/" + folder)
        for file in os.listdir(path):
            if file != ".DS_Store":
                songPath = "Data/genres_split/" + folder + "/" + file
                newSong = AudioSegment.from_wav(path + "/" + file)
                splitSong(t0,t1,newSong,songPath + "0")
                splitSong(t1,t2,newSong,songPath + "1")
                splitSong(t2,t3,newSong,songPath + "3")

# Import the dataset and convert data to spectrograms

import os
import librosa

directory = "Data/genres_split"
x, y = [], []
labels = {}

for i, folder in enumerate(os.listdir(directory)):
  path = directory + '/' + folder
  if folder != '.DS_Store':
    labels[i] = folder
    if not os.path.isdir("Data/spectrograms/" + folder):
      os.mkdir("Data/spectrograms/" + folder)
    for file in os.listdir(path):
      if file != '.DS_Store':
        save_path = "Data/spectrograms/" + folder + '/' + file
        save_path = save_path.replace('.wav', '')
        save_path += '.png'
        audio, sr = librosa.load(path + '/' + file)
        D = np.abs(librosa.stft(audio))**2
        audio = librosa.feature.melspectrogram(y=audio, sr=sr, S=D)
        log_ms = librosa.power_to_db(audio, ref=np.max)
        audio = np.array(log_ms)[:, :428]

        x.append(audio)
        y.append(i)

        if (not os.path.isfile(save_path)):
          fig = plt.figure()
          ax = fig.add_subplot(1, 1, 1)
          fig.subplots_adjust(left=0, right=1, bottom=0, top=1)
          librosa.display.specshow(log_ms, sr=sr)
          plt.close()

          fig.savefig(save_path)

          audio, sr = librosa.load(path + '/' + file)
          D = np.abs(librosa.stft(audio))**2
          audio = librosa.feature.melspectrogram(y=audio, sr=sr, S=D)
          log_ms = librosa.power_to_db(audio, ref=np.max)

x = np.stack(x, axis = 0)

y = np.stack(y, axis = 0)

# Test/Train Split

x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.2, random_state=0)

x_train_norm = np.array(x_train) / 255
x_test_norm = np.array(x_test) / 255

y_train_encoded = to_categorical(y_train)
y_test_encoded = to_categorical(y_test)

# Create Model

import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dropout
from keras.layers import Dense
from keras.layers import TimeDistributed
from keras.layers import Conv1D
from keras.layers import MaxPool1D
from keras.layers import Conv2D
from keras.layers import MaxPool2D
from keras.layers import Flatten
from keras.layers import LeakyReLU
from keras.layers import BatchNormalization
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications import ResNet101
from tensorflow.keras.applications import ResNet50V2
from keras_nlp.layers import TransformerEncoder

model = Sequential()
# model.add(LSTM(10,activation="tanh",return_sequences=True,kernel_regularizer=keras.regularizers.l2(0.035)))
# model.add(Dropout(0.3))
# model.add(LSTM(10,activation="tanh"))
# model.add(Dropout(0.3))
# model.add(Dense(10, activation='relu'))

# model.add(LSTM(128,activation="tanh",return_sequences=True,kernel_regularizer=keras.regularizers.l2(0.035)))
# model.add(Dropout(0.3))
# model.add(LSTM(128,activation="tanh"))
# model.add(Dropout(0.3))
# model.add(Dense(10, activation='relu'))

# model.add(LSTM(32,return_sequences=True,kernel_regularizer=keras.regularizers.l2(0.035)))
# model.add(Dropout(0.3))
# model.add(LSTM(32))
# model.add(Dropout(0.3))
# model.add(Dense(10,activation = "softmax"))

# model.add(TransformerEncoder(128, 10, dropout = 0.5))
# model.add(Flatten())
# model.add(Dense(10, activation="softmax"))

# model.add(Conv1D(128, 3, activation="relu"))
# model.add(MaxPool1D())
# model.add(Dropout(0.3))
# model.add(Conv1D(64, 3, activation="relu"))
# model.add(MaxPool1D())
# model.add(Dropout(0.3))
# model.add(Conv1D(32, 3, activation="relu"))
# model.add(MaxPool1D())
# model.add(Dropout(0.3))
# model.add(Flatten())
# model.add(Dense(10, activation="softmax"))

# model.add(Conv1D(256, 3, activation="relu"))
# model.add(MaxPool1D())
# model.add(Dropout(0.3))
# model.add(Conv1D(128, 3, activation="relu"))
# model.add(MaxPool1D())
# model.add(Dropout(0.3))
# model.add(Conv1D(64, 3, activation="relu"))
# model.add(MaxPool1D())
# model.add(Dropout(0.3))
# model.add(Flatten())
# model.add(Dense(10, activation="softmax"))

# model.add(Conv1D(256, 3, activation="relu"))
# model.add(MaxPool1D())
# model.add(Dropout(0.3))
# model.add(Conv1D(128, 3, activation="relu"))
# model.add(MaxPool1D())
# model.add(Dropout(0.3))
# model.add(Flatten())
# model.add(Dense(10, activation="softmax"))

# model.add(Conv1D(512, 3, activation="relu"))
# model.add(MaxPool1D())
# model.add(Dropout(0.3))
# model.add(Conv1D(256, 3, activation="relu"))
# model.add(MaxPool1D())
# model.add(Dropout(0.3))
# model.add(Flatten())
# model.add(Dense(10, activation="softmax"))

# model.add(Conv2D(128, 3, activation="relu", data_format="channels_last"))
# model.add(MaxPool2D())
# model.add(Dropout(0.5))
# model.add(Conv2D(64, 3, activation="relu", data_format="channels_last"))
# model.add(MaxPool2D())
# model.add(Dropout(0.5))
# model.add(Conv2D(32, 3, activation="relu", data_format="channels_last"))
# model.add(MaxPool2D())
# model.add(Dropout(0.5))
# model.add(Conv2D(16, 3, activation="relu", data_format="channels_last"))
# model.add(MaxPool2D())
# model.add(Dropout(0.5))
# model.add(Conv2D(8, 3, activation="relu", data_format="channels_last"))
# model.add(MaxPool2D())
# model.add(Dropout(0.5))
# model.add(Flatten())
# model.add(Dense(10, activation="softmax"))

# model.add(Conv2D(256, 3, activation="relu", data_format="channels_last"))
# model.add(MaxPool2D())
# model.add(Dropout(0.5))
# model.add(Conv2D(128, 3, activation="relu", data_format="channels_last"))
# model.add(MaxPool2D())
# model.add(Dropout(0.5))
# model.add(Conv2D(64, 3, activation="relu", data_format="channels_last"))
# model.add(MaxPool2D())
# model.add(Dropout(0.5))
# model.add(Conv2D(32, 3, activation="relu", data_format="channels_last"))
# model.add(MaxPool2D())
# model.add(Dropout(0.5))
# model.add(Conv2D(16, 3, activation="relu", data_format="channels_last"))
# model.add(MaxPool2D())
# model.add(Dropout(0.5))
# model.add(Flatten())
# model.add(Dense(10, activation="softmax"))

# model.add(Conv2D(64, 3, activation="relu", data_format="channels_last"))
# model.add(MaxPool2D())
# model.add(Dropout(0.5))
# model.add(Conv2D(32, 3, activation="relu", data_format="channels_last"))
# model.add(MaxPool2D())
# model.add(Dropout(0.5))
# model.add(Conv2D(16, 3, activation="relu", data_format="channels_last"))
# model.add(MaxPool2D())
# model.add(Dropout(0.5))
# model.add(Conv2D(8, 3, activation="relu", data_format="channels_last"))
# model.add(MaxPool2D())
# model.add(Dropout(0.5))
# model.add(Conv2D(4, 3, activation="relu", data_format="channels_last"))
# model.add(MaxPool2D())
# model.add(Dropout(0.5))
# model.add(Flatten())
# model.add(Dense(10, activation="softmax"))

# model.add(Conv2D(256, 3, activation="relu", data_format="channels_last"))
# model.add(LeakyReLU())
# model.add(BatchNormalization())
# model.add(Conv2D(128, 3, activation="relu", data_format="channels_last"))
# model.add(LeakyReLU())
# model.add(BatchNormalization())
# model.add(Flatten())
# model.add(Dense(10, activation="softmax"))

# model.add(Conv1D(512, 3, activation="relu"))
# model.add(LeakyReLU())
# model.add(BatchNormalization())
# model.add(Conv1D(256, 3, activation="relu"))
# model.add(LeakyReLU())
# model.add(BatchNormalization())
# model.add(Flatten())
# model.add(Dense(10, activation="softmax"))

# model.add(Conv1D(1024, 3, activation="relu"))
# model.add(LeakyReLU())
# model.add(BatchNormalization())
# model.add(Conv1D(512, 3, activation="relu"))
# model.add(LeakyReLU())
# model.add(BatchNormalization())
# model.add(Flatten())
# model.add(Dense(10, activation="softmax"))

# model.add(Conv1D(256, 3, activation="relu"))
# model.add(LeakyReLU())
# model.add(BatchNormalization())
# model.add(Conv1D(128, 3, activation="relu"))
# model.add(LeakyReLU())
# model.add(BatchNormalization())
# model.add(Flatten())
# model.add(Dense(10, activation="softmax"))

# model.add(Conv1D(128, 3, activation="relu"))
# model.add(LeakyReLU())
# model.add(BatchNormalization())
# model.add(Conv1D(64, 3, activation="relu"))
# model.add(LeakyReLU())
# model.add(BatchNormalization())
# model.add(Conv1D(32, 3, activation="relu"))
# model.add(LeakyReLU())
# model.add(BatchNormalization())
# model.add(Conv1D(16, 3, activation="relu"))
# model.add(LeakyReLU())
# model.add(BatchNormalization())
# model.add(Conv1D(8, 3, activation="relu"))
# model.add(LeakyReLU())
# model.add(BatchNormalization())
# model.add(Flatten())
# model.add(Dense(10, activation="softmax"))

# model.add(ResNet50(input_shape=x_train_norm.shape[1:], weights=None, classes = 10, classifier_activation="softmax"))

# model.add(ResNet101(input_shape=x_train_norm.shape[1:], weights=None, classes = 10, classifier_activation="softmax"))

# model.add(ResNet50V2(input_shape=x_train_norm.shape[1:], weights=None, classes = 10, classifier_activation="softmax"))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'], run_eagerly=True)

# Train
import gc
gc.collect()
hist = model.fit(x_train_norm, y_train_encoded, validation_data=(x_test_norm, y_test_encoded), batch_size=32, epochs=100)

# Plot Train/Validation Accuracy

acc = hist.history['accuracy']
val_acc = hist.history['val_accuracy']
epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, '-', label='Training Accuracy')
plt.plot(epochs, val_acc, ':', label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.plot()

# Confusion Matrix

from sklearn.metrics import confusion_matrix
import seaborn as sns
sns.set()

y_predicted = model.predict(x_test_norm)
mat = confusion_matrix(y_test_encoded.argmax(axis=1), y_predicted.argmax(axis=1))
class_labels = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']

sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',
            xticklabels=class_labels,
            yticklabels=class_labels)

plt.xlabel('Predicted label')
plt.ylabel('Actual label')